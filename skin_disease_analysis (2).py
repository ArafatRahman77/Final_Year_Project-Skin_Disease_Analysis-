# -*- coding: utf-8 -*-
"""Skin_Disease_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sPa9bmFOqwXXrTfss2yXVWk51rSljVXp
"""

# import os
# import shutil
# import numpy as np
# import tensorflow as tf
# from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
# from sklearn.model_selection import train_test_split
# import matplotlib.pyplot as plt
# from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
# from tensorflow.keras.applications import EfficientNetB0
# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
# from tensorflow.keras.models import Model
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.applications.efficientnet import preprocess_input

# from keras.models import load_model

# #It load the model from the checkpoint
# model = load_model('/content/drive/MyDrive/Final_year_project/best_model.keras')

# # Defined directories
# dataset_dir = '/content/drive/MyDrive/Final_year_project/dataset'
# train_dir = '/content/drive/MyDrive/Final_year_project/train_data'
# val_dir = '/content/drive/MyDrive/Final_year_project/validation_data'
# test_dir = '/content/drive/MyDrive/Final_year_project/test_data'

# # To see if directories exist
# for dir_path in [train_dir, val_dir, test_dir]:
#     os.makedirs(dir_path, exist_ok=True)

# # The list of all files and their corresponding labels
# filepaths = []
# labels = []
# for class_dir in os.listdir(dataset_dir):
#     class_path = os.path.join(dataset_dir, class_dir)
#     for fname in os.listdir(class_path):
#         filepaths.append(os.path.join(class_path, fname))
#         labels.append(class_dir)

# # Splitting into train and temp sets
# train_files, temp_files, train_labels, temp_labels = train_test_split(
#     filepaths, labels, test_size=0.3, stratify=labels)

# # Splitting temp into validation and test sets
# val_files, test_files, val_labels, test_labels = train_test_split(
#     temp_files, temp_labels, test_size=0.5, stratify=temp_labels)

# # Function to check if files exist
# def check_files_exist(filepaths):
#     missing_files = [filepath for filepath in filepaths if not os.path.exists(filepath)]
#     if missing_files:
#         print(f"Missing files: {missing_files}")
#     else:
#         print("All files exist.")
#     return missing_files

# # Check for missing files before copying
# missing_train_files = check_files_exist(train_files)
# missing_val_files = check_files_exist(val_files)
# missing_test_files = check_files_exist(test_files)

# # Remove missing files from lists
# train_files = [f for f in train_files if f not in missing_train_files]
# val_files = [f for f in val_files if f not in missing_val_files]
# test_files = [f for f in test_files if f not in missing_test_files]

# import sys

# # Function to copy files to the destination directory with dynamic progress output
# def copy_files(filepaths, labels, dest_dir):
#     for filepath, label in zip(filepaths, labels):
#         dest_path = os.path.join(dest_dir, label)
#         os.makedirs(dest_path, exist_ok=True)
#         file_name = os.path.basename(filepath)
#         print(f"\rCopying {file_name} to {dest_path}", end="")
#         shutil.copy(filepath, dest_path)
#         sys.stdout.flush()  # Ensure the output is printed immediately

#     print("\nAll files have been copied.")

# # Copy files to respective directories (after removing missing files)
# copy_files(train_files, train_labels, train_dir)
# copy_files(val_files, val_labels, val_dir)
# copy_files(test_files, test_labels, test_dir)

# # # Define image size and batch size
# # IMG_SIZE = (224, 224)  # EfficientNetB0 default input size
# # BATCH_SIZE = 32

# # # Define data generators with EfficientNetB0 preprocessing
# # train_datagen = ImageDataGenerator(
# #     rotation_range=40,
# #     width_shift_range=0.2,
# #     height_shift_range=0.2,
# #     shear_range=0.2,
# #     zoom_range=0.2,
# #     horizontal_flip=True,
# #     fill_mode='nearest',
# #     preprocessing_function=preprocess_input
# # )

# # val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
# # test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

# # # Create data generators
# # train_generator = train_datagen.flow_from_directory(
# #     train_dir,
# #     target_size=IMG_SIZE,
# #     batch_size=BATCH_SIZE,
# #     class_mode='categorical'
# # )

# # val_generator = val_datagen.flow_from_directory(
# #     val_dir,
# #     target_size=IMG_SIZE,
# #     batch_size=BATCH_SIZE,
# #     class_mode='categorical'
# # )

# # test_generator = test_datagen.flow_from_directory(
# #     test_dir,
# #     target_size=IMG_SIZE,
# #     batch_size=BATCH_SIZE,
# #     class_mode='categorical'
# # )

# # # Verify the data generators
# # print(f"Classes: {train_generator.class_indices}")
# # print(f"Number of classes: {len(train_generator.class_indices)}")

# #changed version
# # Define image size and batch size
# # IMG_SIZE = (224, 224)  # EfficientNetB0 default input size
# # BATCH_SIZE = 32

# # Define data generators with EfficientNetB0 preprocessing
# # train_datagen = ImageDataGenerator(
# #     rotation_range=40,
# #     width_shift_range=0.2,
# #     height_shift_range=0.2,
# #     shear_range=0.2,
# #     zoom_range=0.2,
# #     horizontal_flip=True,
# #     fill_mode='nearest',
# #     preprocessing_function=preprocess_input
# # )

# # val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
# # test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

# # # Create data generators
# # train_generator = train_datagen.flow_from_directory(
# #     train_dir,
# #     target_size=IMG_SIZE,
# #     batch_size=BATCH_SIZE,
# #     class_mode='categorical'
# # )

# # val_generator = val_datagen.flow_from_directory(
# #     val_dir,
# #     target_size=IMG_SIZE,
# #     batch_size=BATCH_SIZE,
# #     class_mode='categorical'
# # )

# # test_generator = test_datagen.flow_from_directory(
# #     test_dir,
# #     target_size=IMG_SIZE,
# #     batch_size=BATCH_SIZE,
# #     class_mode='categorical'
# # )

# # # Verify the data generators
# # print(f"Classes: {train_generator.class_indices}")
# # print(f"Number of classes: {len(train_generator.class_indices)}")
# #chaged again:
# # Define image size and batch size
# IMG_SIZE = (224, 224)  # EfficientNetB0 default input size
# BATCH_SIZE = 32

# # Enhanced data augmentation for skin disease analysis
# train_datagen = ImageDataGenerator(
#     rotation_range=45,  # Slightly increased to handle more rotation variations in lesions
#     width_shift_range=0.15,  # Reduced to avoid excessive shifts that may distort lesions
#     height_shift_range=0.15,  # Same as above
#     shear_range=0.1,  # Shear transformations can introduce noise, so kept low
#     zoom_range=0.25,  # Increased zoom to focus on different lesion scales
#     horizontal_flip=True,  # Lesions can be flipped
#     vertical_flip=True,  # Vertical flips can be useful for skin lesion analysis
#     brightness_range=[0.7, 1.3],  # Adjust brightness to simulate various lighting conditions
#     fill_mode='nearest',
#     preprocessing_function=preprocess_input
# )

# # Validation and test generators should not have augmentation, only preprocessing
# val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
# test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

# # Create data generators
# train_generator = train_datagen.flow_from_directory(
#     train_dir,
#     target_size=IMG_SIZE,
#     batch_size=BATCH_SIZE,
#     class_mode='categorical'
# )

# val_generator = val_datagen.flow_from_directory(
#     val_dir,
#     target_size=IMG_SIZE,
#     batch_size=BATCH_SIZE,
#     class_mode='categorical'
# )

# test_generator = test_datagen.flow_from_directory(
#     test_dir,
#     target_size=IMG_SIZE,
#     batch_size=BATCH_SIZE,
#     class_mode='categorical'
# )

# # Verify the data generators
# print(f"Classes: {train_generator.class_indices}")
# print(f"Number of classes: {len(train_generator.class_indices)}")

# # Plot histogram to show distribution of images per class
# def plot_class_distribution(generator):
#     class_counts = {class_name: 0 for class_name in generator.class_indices.keys()}
#     for subdir, _, files in os.walk(generator.directory):
#         if files:
#             class_name = os.path.basename(subdir)
#             if class_name in class_counts:
#                 class_counts[class_name] += len(files)

#     plt.figure(figsize=(10, 6))
#     plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')
#     plt.xlabel('Classes')
#     plt.ylabel('Number of Images')
#     plt.title('Distribution of Images per Class')
#     plt.xticks(rotation=45)
#     plt.tight_layout()
#     plt.show()

# # Display class distribution for training data
# plot_class_distribution(train_generator)

# # Function to display images from the first five folders (classes)
# def display_images_from_folders(base_dir, num_folders=5, num_images_per_folder=3):
#     folders = sorted(os.listdir(base_dir))[:num_folders]

#     plt.figure(figsize=(20, 15))
#     for i, folder in enumerate(folders):
#         folder_path = os.path.join(base_dir, folder)
#         image_files = os.listdir(folder_path)[:num_images_per_folder]

#         for j, image_file in enumerate(image_files):
#             image_path = os.path.join(folder_path, image_file)

#             # Load image before preprocessing
#             img_before = load_img(image_path, target_size=IMG_SIZE)
#             img_before_arr = img_to_array(img_before) / 255.0  # Normalize for display

#             # Load image after preprocessing
#             img_after = load_img(image_path, target_size=IMG_SIZE)
#             img_after_arr = img_to_array(img_after)
#             img_after_arr = preprocess_input(img_after_arr)

#             # Plot images
#             plt.subplot(num_folders, num_images_per_folder * 2, i * num_images_per_folder * 2 + j * 2 + 1)
#             plt.imshow(img_before_arr)
#             plt.title(f'Before Preprocessing\nClass: {folder}')
#             plt.axis('off')

#             plt.subplot(num_folders, num_images_per_folder * 2, i * num_images_per_folder * 2 + j * 2 + 2)
#             plt.imshow((img_after_arr - img_after_arr.min()) / (img_after_arr.max() - img_after_arr.min()))  # Normalize for display
#             plt.title(f'After Preprocessing\nClass: {folder}')
#             plt.axis('off')

#     plt.tight_layout()
#     plt.show()

# # Display first 5 folders and 3 images (before and after preprocessing) from each folder in the training directory
# display_images_from_folders(train_dir, num_folders=5, num_images_per_folder=3)

# # Define the model using EfficientNetB0
# def create_model(num_classes):
#     base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
#     x = base_model.output
#     x = GlobalAveragePooling2D()(x)
#     x = Dropout(0.5)(x)  # Adding Dropout to prevent overfitting
#     outputs = Dense(num_classes, activation='softmax')(x)

#     model = Model(inputs=base_model.input, outputs=outputs)

#     # Compile the model
#     model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

#     return model

# # Get the number of classes
# num_classes = len(train_generator.class_indices)

# # Create model
# model = create_model(num_classes)

# # Check if there is an existing model to resume training from
# model_path = '/content/drive/MyDrive/Final_year_project/best_model.keras'
# if os.path.exists(model_path):
#     print("Loading model from checkpoint...")
#     model = load_model(model_path)
#     initial_epoch = model.optimizer.iterations // (train_generator.samples // BATCH_SIZE)
# else:
#     print("No checkpoint found, initializing new model...")
#     model = create_model(num_classes)
#     initial_epoch = 0

# # Define a custom callback to print metrics after each epoch
# class PrintMetrics(Callback):
#     def on_epoch_end(self, epoch, logs=None):
#         print(f"Epoch {epoch+1} - Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, "
#               f"Val Loss: {logs['val_loss']:.4f}, Val Accuracy: {logs['val_accuracy']:.4f}")

# # Define callbacks
# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)
# model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Final_year_project/best_model.keras', save_best_only=True, monitor='val_loss', mode='min')
# print_metrics = PrintMetrics()

# callbacks = [early_stopping, reduce_lr, model_checkpoint, print_metrics]

# # Train the model
# history = model.fit(
#     train_generator,
#     steps_per_epoch=train_generator.samples // BATCH_SIZE,
#     validation_data=val_generator,
#     validation_steps=val_generator.samples // BATCH_SIZE,
#     epochs=100,
#     callbacks=callbacks
# )

# # # Check if there is an existing model to resume training from
# # model_path = '/content/drive/MyDrive/Final_year_project/best_model.keras'
# # if os.path.exists(model_path):
# #     print("Loading model from checkpoint...")
# #     model = load_model(model_path)
# #     initial_epoch = model.optimizer.iterations // (train_generator.samples // BATCH_SIZE)
# # else:
# #     print("No checkpoint found, initializing new model...")
# #     model = create_model(num_classes)
# #     initial_epoch = 0

# # # Train the model
# # history = model.fit(
# #     train_generator,
# #     steps_per_epoch=train_generator.samples // BATCH_SIZE,
# #     validation_data=val_generator,
# #     validation_steps=val_generator.samples // BATCH_SIZE,
# #     epochs=100,
# #     initial_epoch=initial_epoch,  # Start training from the last epoch
# #     callbacks=callbacks
# # )
# # Define a custom callback to print metrics after each epoch
# class PrintMetrics(Callback):
#     def on_epoch_end(self, epoch, logs=None):
#         print(f"Epoch {epoch+1} - Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, "
#               f"Val Loss: {logs['val_loss']:.4f}, Val Accuracy: {logs['val_accuracy']:.4f}")

# # Define callbacks
# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)
# model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, monitor='val_loss', mode='min')
# print_metrics = PrintMetrics()

# callbacks = [early_stopping, reduce_lr, model_checkpoint, print_metrics]

# # Train the model
# history = model.fit(
#     train_generator,
#     steps_per_epoch=train_generator.samples // BATCH_SIZE,
#     validation_data=val_generator,
#     validation_steps=val_generator.samples // BATCH_SIZE,
#     epochs=100,
#     initial_epoch=initial_epoch,  # Start training from the last epoch
#     callbacks=callbacks
# )

#Model 2 starts from here

# from tensorflow.keras.applications import EfficientNetB0
# from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense
# from tensorflow.keras.models import Model
# from tensorflow.keras.optimizers import Adam

# # Define the model using EfficientNetB0 with a dropout of 0.6
# def create_model_v2(num_classes):
#     base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
#     x = base_model.output
#     x = GlobalAveragePooling2D()(x)
#     x = Dropout(0.6)(x)  # Adjusted dropout value
#     outputs = Dense(num_classes, activation='softmax')(x)

#     model = Model(inputs=base_model.input, outputs=outputs)

#     # Compile the model
#     model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

#     return model



# import os
# from tensorflow.keras.models import load_model

# # Define the path where the model checkpoint will be saved
# model_path = '/content/drive/MyDrive/Final_year_project/best_model_v2.keras'

# # Check if there is an existing model to resume training from
# if os.path.exists(model_path):
#     print("Loading model from checkpoint...")
#     model_v2 = load_model(model_path)
#     # Calculate the initial epoch based on the loaded model
#     initial_epoch = model_v2.optimizer.iterations // (train_generator.samples // BATCH_SIZE)
# else:
#     print("No checkpoint found, initializing new model...")
#     num_classes = len(train_generator.class_indices)
#     model_v2 = create_model_v2(num_classes)
#     initial_epoch = 0

# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback

# # Define a custom callback to print metrics after each epoch
# class PrintMetrics(Callback):
#     def on_epoch_end(self, epoch, logs=None):
#         print(f"Epoch {epoch+1} - Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, "
#               f"Val Loss: {logs['val_loss']:.4f}, Val Accuracy: {logs['val_accuracy']:.4f}")

# # Define callbacks
# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)
# model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, monitor='val_loss', mode='min')
# print_metrics = PrintMetrics()

# callbacks = [early_stopping, reduce_lr, model_checkpoint, print_metrics]

# # Define batch size for training
# BATCH_SIZE = 16  # Adjust as needed

# # Train the model with checkpoint handling
# history_v2 = model_v2.fit(
#     train_generator,
#     steps_per_epoch=train_generator.samples // BATCH_SIZE,
#     validation_data=val_generator,
#     validation_steps=val_generator.samples // BATCH_SIZE,
#     epochs=100,  # Adjust the total number of epochs as needed
#     initial_epoch=initial_epoch,  # Start training from the last epoch
#     callbacks=callbacks
# )

# #Model 3 starts here
# import os
# from tensorflow.keras.models import load_model
# from tensorflow.keras.applications import EfficientNetB0
# from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.models import Model

# # Define the path where the model checkpoint will be saved for Model 3
# model_path = '/content/drive/MyDrive/Final_year_project/best_model_v3.keras'

# # Define the model using EfficientNetB0 with a dropout of 0.7
# def create_model_v3(num_classes):
#     base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
#     x = base_model.output
#     x = GlobalAveragePooling2D()(x)
#     x = Dropout(0.7)(x)  # Adjusted dropout value to 0.7
#     outputs = Dense(num_classes, activation='softmax')(x)

#     model = Model(inputs=base_model.input, outputs=outputs)

#     # Compile the model
#     model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

#     return model

# # Check if there is an existing model to resume training from
# if os.path.exists(model_path):
#     print("Loading model from checkpoint...")
#     model_v3 = load_model(model_path)
#     # Calculate the initial epoch based on the number of samples and batch size
#     initial_epoch = model_v3.optimizer.iterations // (train_generator.samples // BATCH_SIZE)
# else:
#     print("No checkpoint found, initializing new model...")
#     num_classes = len(train_generator.class_indices)
#     model_v3 = create_model_v3(num_classes)
#     initial_epoch = 0  # Start training from the beginning

# # Define the path where the model checkpoint will be saved for Model 3

# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback

# # Define a custom callback to print metrics after each epoch
# class PrintMetrics(Callback):
#     def on_epoch_end(self, epoch, logs=None):
#         print(f"Epoch {epoch+1} - Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, "
#               f"Val Loss: {logs['val_loss']:.4f}, Val Accuracy: {logs['val_accuracy']:.4f}")

# # Define callbacks
# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)
# model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, monitor='val_loss', mode='min')
# print_metrics = PrintMetrics()

# callbacks = [early_stopping, reduce_lr, model_checkpoint, print_metrics]



# # Define batch size for training
# BATCH_SIZE = 32 # Adjust as needed

# # Train the model with checkpoint handling
# history_v3 = model_v3.fit(
#     train_generator,
#     steps_per_epoch=train_generator.samples // BATCH_SIZE,
#     validation_data=val_generator,
#     validation_steps=val_generator.samples // BATCH_SIZE,
#     epochs=100,  # Adjust the total number of epochs as needed
#     initial_epoch=initial_epoch,  # Start training from the last epoch or 0
#     callbacks=callbacks
# )

# #model 5
# from tensorflow.keras.regularizers import l2
# def create_model(num_classes):
#     base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
#     x = base_model.output
#     x = GlobalAveragePooling2D()(x)

#     # Add a dense layer with 512 neurons and ReLU activation
#     x = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(x)
#     x = Dropout(0.5)(x)
#     outputs = Dense(num_classes, activation='softmax')(x)

#     model = Model(inputs=base_model.input, outputs=outputs)

#     model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

#     return model

# num_classes = len(train_generator.class_indices)
# model = create_model(num_classes)

# model_path = '/content/drive/MyDrive/Final_year_project/best_model.keras'

# if os.path.exists(model_path):
#     print("Loading model from checkpoint...")
#     model = load_model(model_path)
#     initial_epoch = model.optimizer.iterations // (train_generator.samples // BATCH_SIZE)
# else:
#     print("No checkpoint found, initializing new model...")
#     model = create_model(num_classes)
#     initial_epoch = 0

# # Define a custom callback to print metrics after each epoch
# class PrintMetrics(Callback):
#     def on_epoch_end(self, epoch, logs=None):
#         print(f"Epoch {epoch+1} - Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, "
#               f"Val Loss: {logs['val_loss']:.4f}, Val Accuracy: {logs['val_accuracy']:.4f}")


# # Callbacks
# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
# model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, monitor='val_loss', mode='min')
# print_metrics = PrintMetrics()
# # Start training the model
# history = model.fit(
#     train_generator,
#     epochs=100,  # Set number of epochs
#     validation_data=val_generator,  # Validation generator for evaluation
#     callbacks=[early_stopping, reduce_lr, model_checkpoint, print_metrics],
#     initial_epoch=initial_epoch
# )

# #Model 6

# import os
# import shutil
# import numpy as np
# import tensorflow as tf
# from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
# from sklearn.model_selection import train_test_split
# import matplotlib.pyplot as plt
# from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
# from tensorflow.keras.applications import EfficientNetB1  # Changed to B1 for better performance
# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
# from tensorflow.keras.models import Model
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.applications.efficientnet import preprocess_input
# from keras.models import load_model

# # Define directories
# dataset_dir = '/content/drive/MyDrive/Final_year_project/dataset'
# train_dir = '/content/drive/MyDrive/Final_year_project/train_data'
# val_dir = '/content/drive/MyDrive/Final_year_project/validation_data'
# test_dir = '/content/drive/MyDrive/Final_year_project/test_data'

# # Ensure directories exist
# for dir_path in [train_dir, val_dir, test_dir]:
#     os.makedirs(dir_path, exist_ok=True)

# # Define image size and batch size
# IMG_SIZE = (224, 224)  # EfficientNetB1 default input size
# BATCH_SIZE = 32

# # Enhanced data augmentation for skin disease analysis
# train_datagen = ImageDataGenerator(
#     rotation_range=45,
#     width_shift_range=0.15,
#     height_shift_range=0.15,
#     shear_range=0.1,
#     zoom_range=0.25,
#     horizontal_flip=True,
#     vertical_flip=True,
#     brightness_range=[0.7, 1.3],
#     fill_mode='nearest',
#     preprocessing_function=preprocess_input
# )

# # Validation and test generators without augmentation
# val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
# test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

# # Create data generators
# train_generator = train_datagen.flow_from_directory(
#     train_dir,
#     target_size=IMG_SIZE,
#     batch_size=BATCH_SIZE,
#     class_mode='categorical'
# )

# val_generator = val_datagen.flow_from_directory(
#     val_dir,
#     target_size=IMG_SIZE,
#     batch_size=BATCH_SIZE,
#     class_mode='categorical'
# )

# test_generator = test_datagen.flow_from_directory(
#     test_dir,
#     target_size=IMG_SIZE,
#     batch_size=BATCH_SIZE,
#     class_mode='categorical'
# )

# # Verify the data generators
# print(f"Classes: {train_generator.class_indices}")
# print(f"Number of classes: {len(train_generator.class_indices)}")

# # Model 6 - Imagenet1
# from tensorflow.keras.regularizers import l2

# def create_model(num_classes):
#     base_model = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(224, 224, 3))  # Changed to EfficientNetB1
#     x = base_model.output
#     x = GlobalAveragePooling2D()(x)

#     # Add a dense layer with L2 regularization to prevent overfitting
#     x = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(x)
#     x = Dropout(0.5)(x)
#     outputs = Dense(num_classes, activation='softmax')(x)

#     model = Model(inputs=base_model.input, outputs=outputs)

#     model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])

#     return model

# # Create model
# num_classes = len(train_generator.class_indices)
# model = create_model(num_classes)

# # Model checkpoint path
# model_path = '/content/drive/MyDrive/Final_year_project/Imagenet1_best_model.keras'

# # Load the model if a checkpoint exists
# if os.path.exists(model_path):
#     print("Loading model from checkpoint...")
#     model = load_model(model_path)
#     initial_epoch = model.optimizer.iterations // (train_generator.samples // BATCH_SIZE)
# else:
#     print("No checkpoint found, initializing new model...")
#     model = create_model(num_classes)
#     initial_epoch = 0

# # Define a custom callback to print both training and validation metrics
# class PrintMetrics(Callback):
#     def on_epoch_end(self, epoch, logs=None):
#         print(f"Epoch {epoch+1} - Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, "
#               f"Val Loss: {logs['val_loss']:.4f}, Val Accuracy: {logs['val_accuracy']:.4f}")

# # Callbacks
# early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)
# model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, monitor='val_loss', mode='min')
# print_metrics = PrintMetrics()

# # Start training the model
# history = model.fit(
#     train_generator,
#     epochs=100,  # Set number of epochs
#     validation_data=val_generator,  # Validation generator for evaluation
#     callbacks=[early_stopping, reduce_lr, model_checkpoint, print_metrics],
#     initial_epoch=initial_epoch
# )

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback
from tensorflow.keras.applications import EfficientNetB2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications.efficientnet import preprocess_input
from keras.models import load_model

# Directories
dataset_dir = '/content/drive/MyDrive/Final_year_project/dataset'
train_dir = '/content/drive/MyDrive/Final_year_project/train_data'
val_dir = '/content/drive/MyDrive/Final_year_project/validation_data'
test_dir = '/content/drive/MyDrive/Final_year_project/test_data'

# Ensure directories exist
for dir_path in [train_dir, val_dir, test_dir]:
    os.makedirs(dir_path, exist_ok=True)

# Image size and batch size
IMG_SIZE = (260, 260)  # EfficientNetB2 default input size
BATCH_SIZE = 32

# Data augmentation
train_datagen = ImageDataGenerator(
    rotation_range=45,
    width_shift_range=0.15,
    height_shift_range=0.15,
    shear_range=0.1,
    zoom_range=0.25,
    horizontal_flip=True,
    vertical_flip=True,
    brightness_range=[0.7, 1.3],
    fill_mode='nearest',
    preprocessing_function=preprocess_input
)

# Validation and test generators
val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

# Data generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

# class details
print(f"Classes: {train_generator.class_indices}")
print(f"Number of classes: {len(train_generator.class_indices)}")

# Model creation
from tensorflow.keras.regularizers import l2

def create_model(num_classes):
    base_model = EfficientNetB2(weights='imagenet', include_top=False, input_shape=(260, 260, 3))
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(x)
    x = Dropout(0.5)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    model = Model(inputs=base_model.input, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Model path
model_path = '/content/drive/MyDrive/Final_year_project/Imagenet2_best_model.keras'

# Load existing model or create a new one
if os.path.exists(model_path):
    print("Loading model from checkpoint...")
    model = load_model(model_path)
    initial_epoch = model.optimizer.iterations // (train_generator.samples // BATCH_SIZE)
else:
    print("No checkpoint found, initializing new model...")
    num_classes = len(train_generator.class_indices)
    model = create_model(num_classes)
    initial_epoch = 0

# Custom callback to print metrics
class PrintMetrics(Callback):
    def on_epoch_end(self, epoch, logs=None):
        print(f"Epoch {epoch+1} - Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, "
              f"Val Loss: {logs['val_loss']:.4f}, Val Accuracy: {logs['val_accuracy']:.4f}")

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)
model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, monitor='val_loss', mode='min')
print_metrics = PrintMetrics()

# Train the model
history = model.fit(
    train_generator,
    epochs=130,
    validation_data=val_generator,
    callbacks=[early_stopping, reduce_lr, model_checkpoint, print_metrics],
    initial_epoch=initial_epoch
)

# Evaluate on the test set
test_loss, test_accuracy = model.evaluate(test_generator)
print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

# import pickle

# # Save model as pkl
# model_dict = {
#     "architecture": model.to_json(),  # Save model architecture
#     "weights": model.get_weights()    # Save model weights
# }

# pkl_path = "/content/drive/MyDrive/Final_year_project/model.pkl"
# with open(pkl_path, "wb") as f:
#     pickle.dump(model_dict, f)

# print(f"Model saved as {pkl_path}")

import os

# Define dataset directory
dataset_dir = "/content/drive/MyDrive/Final_year_project/train_data"  # Change this to your directory

# Get class names (folder names)
class_names = [folder for folder in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, folder))]

# Print class names
print("Class Names:", class_names)
print("Number of Classes:", len(class_names))

# import os
# import numpy as np
# import tensorflow as tf
# import matplotlib.pyplot as plt
# from tensorflow.keras.preprocessing import image
# from google.colab import files
# from keras.models import load_model

# # Define paths
# model_path = "/content/drive/MyDrive/Final_year_project/Imagenet2_best_model.keras"
# dataset_dir = "/content/drive/MyDrive/Final_year_project/train_data"

# # Load class names
# class_names = [folder for folder in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, folder))]
# class_names.sort()  # Ensure class order is consistent

# # Load trained model
# model = load_model(model_path)

# # Function to preprocess image
# def preprocess_img(img_path, img_size=(260, 260)):
#     img = image.load_img(img_path, target_size=img_size)
#     img_array = image.img_to_array(img)
#     img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
#     img_array = tf.keras.applications.efficientnet.preprocess_input(img_array)  # Normalize
#     return img_array

# # Function to predict class
# def predict_image(img_path):
#     img_array = preprocess_img(img_path)
#     predictions = model.predict(img_array)[0]  # Get prediction
#     predicted_class = np.argmax(predictions)
#     confidence = predictions[predicted_class] * 100

#     print(f"Predicted Class: {class_names[predicted_class]}")
#     print(f"Confidence: {confidence:.2f}%")

#     # Show class-wise accuracy
#     print("\nClass-wise Probabilities:")
#     for i, (cls, prob) in enumerate(zip(class_names, predictions)):
#         print(f"{cls}: {prob * 100:.2f}%")

#     # Display image
#     plt.imshow(image.load_img(img_path))
#     plt.axis("off")
#     plt.title(f"Prediction: {class_names[predicted_class]} ({confidence:.2f}%)")
#     plt.show()

# # Upload and classify image
# uploaded = files.upload()

# # Get the uploaded file name
# uploaded_filename = list(uploaded.keys())[0]

# # Run prediction
# predict_image(uploaded_filename)

print("Class Labels and Corresponding Names:")
for i, cls in enumerate(class_names):
    print(f"Label {i}: {cls}")

# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.metrics import confusion_matrix, classification_report

# # Get class labels
# class_names = list(test_generator.class_indices.keys())

# # Get true labels and predictions
# y_true = []
# y_pred = []

# for batch_images, batch_labels in test_generator:
#     batch_preds = model.predict(batch_images)  # Predict
#     batch_preds = np.argmax(batch_preds, axis=1)  # Convert to class indices
#     batch_labels = np.argmax(batch_labels, axis=1)  # True labels

#     y_true.extend(batch_labels)
#     y_pred.extend(batch_preds)

#     if len(y_true) >= test_generator.samples:  # Stop when all test images are processed
#         break

# y_true = np.array(y_true)
# y_pred = np.array(y_pred)

# # Compute confusion matrix
# cm = confusion_matrix(y_true, y_pred)

# # Compute class-wise accuracy
# class_accuracies = cm.diagonal() / cm.sum(axis=1)

# # Print class-wise accuracy
# print("\nClass-wise Accuracy:")
# for i, acc in enumerate(class_accuracies):
#     print(f"{class_names[i]}: {acc * 100:.2f}%")

# # Plot class-wise accuracy histogram
# plt.figure(figsize=(12, 6))
# plt.bar(class_names, class_accuracies * 100, color='skyblue')
# plt.xlabel("Classes")
# plt.ylabel("Accuracy (%)")
# plt.title("Class-wise Accuracy")
# plt.xticks(rotation=90)
# plt.ylim(0, 100)
# plt.grid(axis="y", linestyle="--", alpha=0.7)
# plt.show()

# # Compute overall test accuracy
# overall_accuracy = np.mean(y_true == y_pred)
# print(f"\nOverall Test Accuracy: {overall_accuracy * 100:.2f}%")

# # Plot confusion matrix
# plt.figure(figsize=(10, 8))
# sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
# plt.xlabel("Predicted Labels")
# plt.ylabel("True Labels")
# plt.title("Confusion Matrix")
# plt.show()